# -*- coding: utf-8 -*-
"""BreastCancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d7euZ1vkQLt413nr2g_J2VaZGpfJIdvK
"""

import pandas as pd
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

df = pd.read_csv("/content/breast_cancer.csv")

df.head()

df.describe()

#checking for missing values
df.isnull().sum()

msno.bar(df, color="red")

#This column contains categorical values indicating the diagnosis, such as 'M' for malignant and 'B' for benign
df['diagnosis'] = df['diagnosis'].apply(lambda val:1 if val=='M' else 0)

plt.hist(df['diagnosis'])
plt.title('Diagnosis(M=1, B=0)')
plt.show()

#Creating an EDA(Exploratory Data Analysis) in order to understand the behavior of each feature in the dataset
plt.figure(figsize=(20,15))
plotnumber=1
for column in df:
    if plotnumber<=30:
        ax = plt.subplot(5,6, plotnumber)
        sns.distplot(df[column])
        plt.xlabel(column)
    plotnumber+=1

plt.tight_layout()
plt.show()

df.corr()

#how to perform a feature selection, as we do have a lot of features and need to specify what exactly would be useful
corr_matrix = df.corr().abs() #calculating the correlation matrix and knwoing the absolute values because we want to know thw strong positives and the strong negatives
mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) #np.triu(...): This function generates an upper triangular matrix, meaning it retains the values above the diagonal and sets the rest to False.
tri_df = corr_matrix.mask(mask)

to_drop = [x for x in tri_df.columns if any(tri_df[x]>0.92)]

df = df.drop(to_drop, axis=1)

print(df.shape[1])

df.head()

X=df.drop('diagnosis', axis=1)
y=df['diagnosis']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train ,y_test =train_test_split(X,y, test_size=0.2, random_state=0)

# scaling data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train.shape

print(df.isna().sum())

has_nan = df.isna().any().any()
print(has_nan)

df.drop('points_worst', axis=1, inplace=True)

df.drop('symmetry_worst', axis=1, inplace=True)

df.drop('fractal_dimension_worst', axis=1, inplace=True)

print(df.isna().sum())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_

final_predictions = best_model.predict(X_test)

print(confusion_matrix(y_test, final_predictions))
print(classification_report(y_test, final_predictions))

print(final_predictions)

